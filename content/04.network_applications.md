## Applying Knowledge Graphs to Biomedical Challenges

1. Mention that these graphs can be used for discovery
2. Mention representation learning (aka representing a graph as dense vectors for nodes and/or edges)
3. 

### Unifying Techniques

1. Set up the problem that maps a knowledge graph into a low dimensional space

#### Matrix Factorization

Matrix factrorization is a technique that uses linear algebra to decompose a single matrix into smaller rectangular matrices.
The resulting rectangular matrices represent nodes captured in a low dimensional space.
Methods in this category rely on a network's adjacency matrix or matrices of similar form for input.
Notable methods for decomposing a matrix are: Isomap [@doi:10.1126/science.290.5500.2319], Laplacian eigenmaps [@doi:10.7551/mitpress/1120.003.0080], PCA [@doi:10.1016/0169-7439(87)80084-9] or SVD [@doi:10.1007/BF02288367] and Multidimesional scaling (MDS) [@doi:10.1007/978-3-540-33037-0_14].
In this section we cover approaches that previously mentioned methods to project networks into a low dimensional space.

Gong et al. [@raw:gongreplearn] used a supervised algorithm called signed laplacian embeddings to project a network into a low dimensional space.
Given labels for each node, they defined an adjacency matrix with positive (1) and negative values (-1).
The positive values represent edges that have nodes from the same class being connected together, which the negative values represent nodes from different classes.
Upon construction of this matrix the authors constructed their transformation matrix using eigenvalues.
Other methods [@doi:10.1109/TKDE.2016.2606098; @raw:hanreplearn] used a laplacian approach to project networks into a low dimensional space.
Despite the success of these methods, they rely on a linear mapping function which may not be appropriate given certain network architectures.

Other works [@doi:10.1145/2806416.2806512; @raw:zhaoreplearn; @raw:yangreplearn; @raw:tureplearn] equate their matrix factorization method with the popular Deepwalk method [@arxiv:1403.6652]. 
These methods simulate the random walk that Deepwalk uses via matrix multiplications.
The adjacency matrix is multiplied with a normalized degree matrix to represent transition probabilities.
This construct matrix turns the process of random walks into simple matrix multiplication.
Lastly these methods use a form of matrix factorization on to project the nodes into a low dimensional space.

Overall matrix factorization is a powerful technique that uses a matrix similar to an adjacency matrix as input.
Despite reported success, these methods work only for regular size networks.
If a network were to expand to gargantuan size, these  approaches wouldn't be machine viable.
Plus, these methods do not take edge type into account.
For homogenous networks this isn't a problem, but for heterogeneous networks information can be lost.  

#### Deep Learning

1. Define node neighborhoods
2. Talk about random walks 
3. Talk about auto encoders random walk independent approaches 

### Unifying Applications

1. Mention how the previous section is used in a biomedical setting

#### Disease and Gene Interactions

1. Mention disease gene prioritization
2. Mention Disease gene associations

#### Protein Protein Interactions

1. Mention predicting genes interacting genes

#### Drug Interactions

1. Talk about drug side effects
2. Drug repurposing
3. Drug-Disease Interations

#### Clinical applications

1. Can mention EHR use and other related applications
2. Mention Tiffany's work on private data embeddings
