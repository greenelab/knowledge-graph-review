## Applying Knowledge Graphs to Biomedical Challenges

1. Mention that these graphs can be used for discovery
2. Mention representation learning (aka representing a graph as dense vectors for nodes and/or edges)
3. 

### Unifying Techniques

Mapping high dimensional data into a low dimensional space has greatly improved modeling performance in fields such as natural language processing [@arxiv:1301.3781; @arxiv:1310.4546] and image analysis [@arxiv:1512.03385].
The success of these approaches provides rationale for projecting knowledge graphs into a low dimensional space as well [@arxiv:1709.05584].
Techniques that perform this projection often require information on how nodes are connected with one another [@raw:gongreplearn; @doi:10.1109/TKDE.2016.2606098; @raw:hanreplearn; @doi:10.1145/2806416.2806512], while other approaches can work directly with the edges themselves [@arxiv:1610.04073].
We group methods for producing low-dimensional representations of knowledge graphs into the following three categories: matrix factorization, translational methods, and deep learning.

#### Matrix Factorization

Matrix factorization is a technique that uses linear algebra to map high dimensional data into a low dimensional space.
This projection is accomplished by decomposing a matrix into a set of small rectangular matrices.
In the context of knowledge graphs the input matrix would be an adjacency matrix where the rows and columns are nodes and each entry in this matrix represents the presence of an edge between two nodes.
The output of this technique would be a small matrix where each row represents a low dimensional embedding for every node in the adjacency matrix.
Notable methods that perform this projection are Isomap [@doi:10.1126/science.290.5500.2319], Laplacian eigenmaps [@doi:10.7551/mitpress/1120.003.0080] and PCA [@doi:10/bm8dnf] or SVD [@doi:10.1007/BF02288367].
In this section we discuss selected approaches that utilize the above methods to project knowledge graphs into a low dimensional space.

Laplacian eigenmap is an unsupervised approach that learns the local structure of a knowledge graph.
Given an adjacency matrix (A) this algorithm first calculates a laplacian matrix.
A laplacian matrix (L) is a matrix that is calculated by subtracting a knowledge graph's degree matrix (D) (a diagonal matrix where each entry represents the number of edges connected to a node) from its adjacency matrix ($L = D-A$).
Using the laplacian matrix, this algorithm then calculates eigenvalues and eigenvectors ($Lx = \lambda Dx$).
The calculated eigenvectors represent the nodes of the knowledge graph [@doi:10.7551/mitpress/1120.003.0080].
A number of approaches have used variants of this algorithm to perform their own node projection [@raw:gongreplearn; @doi:10.1109/TKDE.2016.2606098; @raw:hanreplearn].
This type of methodology is easy to execute the only input required is an adjacency matrix and a degree matrix; however, this approach relies on a linear mapping function, which may not be appropriate given certain biomedical problems.

Following Laplacian eigenmaps, other works constructed variants of the adjacency matrix before undergoing matrix factorization [@doi:10.1145/2806416.2806512; @raw:zhaoreplearn; @raw:yangreplearn; @raw:tureplearn].
These methods simulate the random walk that Deepwalk uses via matrix multiplications.
The adjacency matrix is multiplied with a normalized degree matrix to represent transition probabilities.
This construct matrix turns the process of random walks into simple matrix multiplication.
Lastly these methods use a form of matrix factorization on to project the nodes into a low dimensional space.

Overall matrix factorization is a powerful technique that uses a matrix similar to an adjacency matrix as input.
Despite reported success, these methods work only for regular size knowledge graphs.
If a knowledge graph were to expand to gargantuan size, these type of approaches would become impractical.
Furthermore, these methods treat all edge types the same, which may not be appropriate for knowledge graphs with heterogeneous edge types.

#### Translational Distance Models

Translational distance models treat edges in a knowledge graph as linear transformations.
As an example, one such algorithm, TransE [@raw:bordestranse], treats every node-edge pair as a triplet with head nodes represented as $\textbf{h}$, edges represented as $\textbf{r}$, and tail nodes represented as $\textbf{t}$.
These representations are combined into an equation that mimics the iconic word vectors translations ($\textbf{king} - \textbf{man} + \textbf{woman} \approx \textbf{queen}$) from the Word2vec model [@arxiv:1310.4546].
The equation is shown as follows: $\textbf{h} + \textbf{r} \approx \textbf{t}$.
Starting at the head node ($\textbf{h}$), add the edge vector ($\textbf{r}$) and the result should be the tail node ($\textbf{t}$).
TransE optimizes embeddings for $\textbf{h}$, $\textbf{r}$, $\textbf{t}$, while guaranteeing the global equation ($\textbf{h} + \textbf{r} \approx \textbf{t}$) is satisfied [@raw:bordestranse].
A caveat to the TransE approach is that it the training steps force relationships to have a one to one mapping, which may not be appropriate for all types of relationships.

Wang et al. [@raw:wangtransH] attempted to resolve the one to one mapping issue by developing the TransH model.
TransH treats relations as hyperplanes rather than a regular vector and projects the head ($\textbf{h}$) and tail ($\textbf{t}$) nodes onto the hyperplane.
Following this projection, a distance vector ($\textbf{d}_{r}$) is calculated between the projected head and tail nodes.
Finally, each vector is optimized while preserving the global equation ($\textbf{h} + \textbf{d}_{r} \approx \textbf{t}$) [@raw:wangtransH].
Other approaches [@raw:lintransR, @arxiv:1610.04073; @arxiv:1909.00672] have built off of the TransE and TransH models. 
In the future, it may be beneficial for these models is to incorporate other types of information such as edge confidence scores, textual information, or edge type information when optimizing these embeddings.

#### Deep Learning

1. Define node neighborhoods
2. Talk about random walks 
3. Talk about auto encoders random walk independent approaches 

### Unifying Applications

Knowledge graphs have been used in many biomedical applications ranging from identifying protein functions [@doi:10.1186/s12859-018-2163-9] to prioritizing cancer genes [@doi:10.1093/bioinformatics/bty148] to recommending safer drugs to patients [@arxiv:1710.05980; @doi:10.1609/aaai.v33i01.33011126].
In this section we discuss how knowledge graphs are being applied in biomedical settings. 
We put particular emphasis on an emerging set of techniques: those that project knowledge graphs into a low dimensional space.

#### Disease and Gene Interactions

1. Mention disease gene prioritization
2. Mention Disease gene associations

#### Protein Protein Interactions

1. Mention predicting genes interacting genes

#### Drug Interactions

1. Talk about drug side effects
2. Drug repurposing
3. Drug-Disease Interations

#### Clinical applications

1. Can mention EHR use and other related applications
2. Mention Tiffany's work on private data embeddings
